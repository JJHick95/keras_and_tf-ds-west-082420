{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Tensorflow and Keras"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot as plt\n", "\n", "\n", "# This is always a good idea\n", "%load_ext autoreload\n", "%autoreload 2\n", "    \n", "from src.student_caller import one_random_student, three_random_students\n", "from src.student_list import student_first_names\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "T8EQBfbRRV7w"}, "source": ["## Modeling\n", "\n", "Let's review some modeling concepts we've used to date with [this quick exercise](https://forms.gle/yrPxUp2Xj4R9FeyEA)\n"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Hko6Q989RV7w"}, "source": ["We do this to remind ourselves that the basic components of good modeling practice, and even the methods themselves, are _the same_ with Neural Nets as that are with _sklearn_ or _statsmodels_.\n", "\n", "The above exercise uses only one train-test split, but is still useful.  We will be using train, validation, test in this notebook, for good practice."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Xv_Ypg88RV7x"}, "source": ["## Objectives:\n", "- Compare pros and cons of Keras vs TensorFlow\n", "- hands on practice coding a neural network"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import keras"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wait a second, what is that warning? \n", "`Using TensorFlow backend.`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img align =left src=\"img/keras.png\"><br>\n", "### Keras is an API\n", "\n", "Coded in Python, that can be layered on top of many different back-end processing systems.\n", "\n", "![kerasback](img/keras_tf_theano.png)\n", "\n", "While each of these systems has their own coding methods, Keras abstracts from that in streamlined pythonic manner we are used to seeing in other python modeling libraries.\n", "\n", "Keras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as tf.keras. Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is maintaining the Keras fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).\n", "\n", "Theano has been discontinued.  The last release was 2017, but can still be used.\n", "\n", "We will use TensorFlow, as it is the most popular. TensorFlow became the most used Keras backend, and  eventually integrated Keras into via the tf.keras submodule of TensorFlow.  "]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "EdZDyJfARV8l"}, "source": ["## Wait, what's TensorFlow?\n"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "mFZYNoYRRV8l"}, "source": ["## Let's start with tensors"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "mIZD7WFvRV8m"}, "source": ["## Tensors are multidimensional matricies\n", "\n", "![tensor](img/tensors.png)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "OUblayDlRV8n"}, "source": ["### TensorFlow manages the flow of matrix math\n", "\n", "That makes neural network processing possible.\n", "\n", "![cat](img/cat-tensors.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For our numbers dataset, our tensors from the sklearn dataset were originally tensors of the shape 8x8, i.e.64 pictures.  Remember, that was with black and white images.\n", "\n", "For image processing, we are often dealing with color."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_sample_images\n", "image = load_sample_images()['images'][0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import matplotlib.image as mpimg\n", "\n", "imgplot = plt.imshow(image)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["image.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What do the dimensions of our image above represent?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(student_first_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tensors with higher numbers of dimensions have a higher **rank**, in the language of TensorFlow."]}, {"cell_type": "markdown", "metadata": {}, "source": ["A matrix with rows and columns only, like the black and white numbers, are **rank 2**.\n", "\n", "A matrix with a third dimension, like the color pictures above, are **rank 3**.\n", "\n", "When we flatten an image by stacking the rows in a column, we are decreasing the rank. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["flat_image = image.reshape(-1,1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When we unrow a column, we increase its rank."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["unrowed = flat_image.reshape(427,640, -1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imgplot = plt.imshow(unrowed)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## TensorFLow has more levers and buttons, but Keras is more user friendly\n", "\n", "Coding directly in **Tensorflow** allows you to tweak more parameters to optimize performance. The **Keras** wrapper makes the code more accessible for developers prototyping models.\n", "\n", "![levers](img/levers.jpeg)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "nTxfhoqMRV8o"}, "source": ["### Keras, an API with an intentional UX\n", "\n", "- Deliberately design end-to-end user workflows\n", "- Reduce cognitive load for your users\n", "- Provide helpful feedback to your users\n", "\n", "[full article here](https://blog.keras.io/user-experience-design-for-apis.html)<br>\n", "[full list of why to use Keras](https://keras.io/why-use-keras/)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "qTNbwNeRRV8p"}, "source": ["### A few comparisons\n", "\n", "While you **can leverage both**, here are a few comparisons.\n", "\n", "| Comparison | Keras | Tensorflow|\n", "|------------|-------|-----------|\n", "| **Level of API** | high-level API | High and low-level APIs |\n", "| **Speed** |  can *seem* slower |  is a bit faster |\n", "| **Language architecture** | simple architecture, more readable and concise | straight tensorflow is a bit more complex |\n", "| **Debugging** | less frequent need to debug | difficult to debug |\n", "| **Datasets** | usually used for small datasets | high performance models and large datasets that require fast execution|\n", "\n", "This is also a _**non-issue**_ - as you can leverage tensorflow commands within keras and vice versa. If Keras ever seems slower, it's because the developer's time is more expensive than the GPUs. Keras is designed with the developer in mind. \n", "\n", "\n", "[reference link](https://www.edureka.co/blog/keras-vs-tensorflow-vs-pytorch/)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Now let's get our feet wet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's import the numbers dataset we used this morning."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_digits\n", "from sklearn.model_selection import train_test_split\n", "digits = load_digits()\n", "X = digits.data\n", "y = digits.target\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Getting data ready for modeling\n", "**Preprocessing**:\n", "\n", "- use train_test_split to create X_train, y_train, X_test, and y_test\n", "- Split training data into train and validation sets.\n", "- Scale the pixel intensity to a value between 0 and 1.\n", "- Scale the pixel intensity to a value between 0 and 1."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Scaling our input variables will help speed up our neural network [see 4.3](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n", "\n", "Since our minimum intensity is 0, we can normalize the inputs by dividing each value by the max value (16). "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.2)\n", "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state=42, test_size=.2)\n", "X_t, X_val, X_test = X_t/16, X_val/16, X_test/16\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that our data is ready, let's load in the keras Sequential class.  \n", "\n", "In this lesson, we are only proceeding with feed forward models.  Our network proceeds layer by layer in sequence.\n", "\n", "Sequential refers to a sequence of layers that feed directly into one another with exactly [one input tensor and one output tensor](https://www.tensorflow.org/guide/keras/sequential_model)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import the Sequential model from the keras.models module\n", "\n", "# Instantiate an instance of the Sequential model with the variable name model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we want to specify the type for our first hidden layer.\n", "\n", "To begin, we will only deal with dense layers.  Remember, dense means fully connected.  Every neuron passes a signal to every neuron in the next layer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import Dense from keras.layers\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["As we will see, building neural networks is a highly empirical process.  There are numerous architectural choices to be made whose impact is hard to predict.  One has to proceed systematically, keeping track of the changes to the architure made along the way, tweeking the hyperparameters and layers until a good model is found.\n", "\n", "That being said, some aspects of our model require specific components. \n", "\n", "For our first hidden layer, we need to specify both the number of neurons in the layer, the activation function, and the dimensions of our input."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Out of those three choices, which is non-negotiable?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(student_first_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# We use model.add to add a layer.  Pass Dense() to model.add, with the parameters specified above\n", "# hints: 'sigmoid', 'input_dim'\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we have to specify our output layer.\n", "\n", "To do so, we have to choose an appropriate activation function which mirrors the sample space of our potential outcomes.\n", "\n", "What activation should we use for our output layer?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(student_first_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add another layer with the appropriate units and activation function. \n", "# We use the same syntax as the first hidden layer\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lastly, for this simple model, we have to define a loss function, a metric, and an optimizer.\n", "\n", "Optimizers are functions which update our weights in smart ways instead of treating all parameters equaly. Adam, a popular optimizer, calculates an individual learning rate for each parameter. Here is a list of available optimizers in Keras: [optimizers](https://keras.io/api/optimizers/)\n", "\n", "We specify these parameters in the compile method."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looking back at this morning's lecture, what loss function should we use?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["one_random_student(student_first_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# call compile on the model and pass as argument for the appropriate loss, \n", "# an optimizer 'adam', and metrics as a string passed to a list\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can fit our model in a similar way as we did our sklearn models, using a .fit method.\n", "\n", "Before we do so, we have to convert out target values with a One Hot Encoder, which is the form Keras requires. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# pass y_t.reshape(-1,1) into the fit_transform method of a OneHotEncoder\n", "\n", "y_t_ohe = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# pass X_t and y_t_ohe to the model's fit method\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["How did we do? Keras behaves in a way which makes replication across computers difficult, even if we were to add a random seed.  In other words may get slightly varying results."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The model once fit now has the ability to both predict and predict_classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Try both methods out with X_val"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Instead of checking the performance on val each time with the above methods, we can score our validation data along with the training data by passing it as a tuple as the validation_data parameter in our .fit \n", "\n", "But first, we have to transform y_val like we did y_t."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Transform y_val with the ohe object from above"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Now, call the .fit() method again with the validation data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["How did we do on the validation data?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have our input and output layers set, let's try to boost our accuracy.\n", "\n", "To begin, let's allow our algorithm to train for a longer.  To do so, we increase the epochs using the `epochs` parameter in .fit(). Let's change it to 5."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Increase our epochs to 5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now our loss is going down and accuracy is going up a bit. \n", "\n", "Let's plot our loss across epochs. In order to do that, we have to store the results of our model.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# instead of simply calling model.fit(), assign it to a variable results\n", "\n", "results = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Now take a look at results.history\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can track the loss and accuracy from each epoch to get a sense of our model's progress."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# We can now plot our loss and accuracy against results.epoch\n", "\n", "def plot_results(results):\n", "    \n", "    '''\n", "    After running a model, plot the training and validation\n", "    scores for both loss and accuracy.\n", "    '''\n", "    \n", "    val_loss = results.history['val_loss']\n", "    train_loss = results.history['loss']\n", "\n", "    val_accuracy = results.history['val_accuracy']\n", "    train_accuracy = results.history['accuracy']\n", "    \n", "    fig, ax = plt.subplots(1,2, figsize=(10,7))\n", "\n", "    ax[0].plot(results.epoch, val_loss, c='b', label='val')\n", "    ax[0].plot(results.epoch, train_loss, c='r', label='train')\n", "    ax[0].legend()\n", "    ax[0].set_title('Model Loss')\n", "\n", "    ax[1].plot(results.epoch, val_accuracy, c='b', label='val')\n", "    ax[1].plot(results.epoch, train_accuracy, c='r', label='train')\n", "    ax[1].set_title('Model Accuracy')\n", "\n", "    ax[1].legend()\n", "    \n", "\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our goal in modeling is to minimize the loss while maximizing accuracy. Remember, our models don't actually optimize for the metric we assign: they learn by minimizing the loss.  We can get a sense as to whether our model has converged on a minimim by seeing whether our loss has stopped decreasing.  The plots of epochs vs loss will level off."]}, {"cell_type": "markdown", "metadata": {}, "source": ["With this goal in mind, let's start testing out some different architectures and parameters.\n", "Remember, this is an empirical process.  We experiment with educated guesses as to what may improve our model's performance.\n", "\n", "A first logical step would be to allow our model to learn for a longer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Here is the current state of our model, along with all the code we need to make our model run.\n", "\n", "# Let's see what happens if we up our fit to 10 epochs\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from sklearn.preprocessing import OneHotEncoder\n", "\n", "ohe = OneHotEncoder(sparse=False)\n", "y_t_ohe = ohe.fit_transform(y_t.reshape(-1,1))\n", "y_val_ohe = ohe.transform(y_val.reshape(-1,1))\n", "\n", "model = Sequential()\n", "model.add(Dense(units=4, activation='relu', input_dim=64))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Instead of adding more epochs, let's deepen our network by adding another hidden layer. It is a good idea to try out deep networks, since we know that successive layers find increasingly complex patterns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(units=4, activation='relu', input_dim=64))\n", "# add another Dense layer. Try out 6 units and 'relu' activation\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=10)\n", "plot_results(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Try out the tanh activation function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "# try out the tanh activation\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=10)\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Without knowing, we have been performing batch gradient descent.  Let's try out mini-batch gradient descent.\n", "\n", "To do so, we add a batch size to our fit.\n", "\n", "As a favorite blogger, Jason Brownlee suggests:\n", "\n", "Mini-batch sizes, commonly called \u201cbatch sizes\u201d for brevity, are often tuned to an aspect of the computational architecture on which the implementation is being executed. Such as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on. [source](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "# Add a batch_size to our fit. Try 32\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=10)\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also try true stochastic gradient descent by specifying 1 as the batch size."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(units=4, activation='relu', input_dim=64))\n", "model.add(Dense(units=6, activation='relu'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"SGD\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "# for Stochastic gradient descent, add a batch size of 1\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=10)\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since a SGD batchsize seems to work well, but takes a relatively long time, let's try a slightly bigger batch size of 5 and boost the epochs to 50.  Hopefully, this will allow us to achieve similarly good results as SGD, but in a reasonable amount of time."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "opt = keras.optimizers.Adam(learning_rate=0.01)\n", "\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n", "# Change batch size to 5 and epochs to 50\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=50, batch_size=5)\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we're talking.  We are beginnig to see a leveling off of our validation loss, indicating that we may be close to converging on a minimum.\n", "\n", "But as you may notice, the validation loss is beginning to separate from the training loss.\n", "\n", "Let's run this a bit longer, and see what happens"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "# Up the epochs to 150\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=50, batch_size=5)\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If this model is behaving at all like it was for me last night, we are beginning to experience some overfitting.  Your val loss may have even started increasing."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Regularization"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order to combat overfitting, we have several regularization techniques to employ.  In the present case, the most intuitive choice is early stopping.\n", "\n", "## Early Stopping\n", "For early stopping, we allow our model to run until some condition is met. \n", "\n", "One practical way to do this is monitoring the validation loss. \n", "\n", "We can set-up early stopping to stop our model whenever it sees an increase in validation loss by setting min_delta to a very low number and patience to 0.  Increasing the patience waits a specified number of epochs without improvement of the monitored value.  Increasing the patience in effect allows for abberations protecting against the case that a given epoch, by random chance, led to a worse metric.  If we see a decrease in score across multiple epochs in a row, we can be fairly certain more training of our network will only result in overfitting."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras.callbacks import EarlyStopping\n", "\n", "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=0, verbose=1,\n", "                           mode='min')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "\n", "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=0, verbose=1,\n", "                           mode='min')\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=500, batch_size=32, callbacks=[early_stop])\n", "plot_results(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "\n", "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=5, verbose=1,\n", "                           mode='min')\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=500, batch_size=32, callbacks=[early_stop])\n", "plot_results(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Drop Out Layers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Although the final two regularization techniques make less sense in our present case, since overfitting only occurs late in our training, we have two other common regularization techniques."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can add dropout layers to our model.  \n", "\n", "We can specify a dropout layer in keras, which randomly shuts off different nodes during training.\n", "\n", "![drop_out](img/drop_out.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras.layers import Dropout\n", "model = Sequential()\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dropout(rate=.25))\n", "model.add(Dense(units=6, activation='tanh'))\n", "model.add(Dense(units=10, activation='softmax'))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe), epochs=500)\n", "plot_results(results)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also add L1 and L2 regularization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras import regularizers\n", "\n", "from keras.layers import Dropout\n", "model = Sequential()\n", "model.add(Dense(units=4, activation='tanh', input_dim=64))\n", "model.add(Dense(20 ,  activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n", "model.add(Dense(units=6, activation='tanh', kernel_regularizer=regularizers.l2(0.01)) )\n", "model.add(Dense(units=10, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n", "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n", "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=10, verbose=1,\n", "                           mode='min')\n", "results = model.fit(X_t, y_t_ohe, validation_data=(X_val, y_val_ohe),\n", "                    epochs=500, batch_size=64, callbacks=[early_stop])\n", "plot_results(results)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# need to add learning rate"]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env_2-keras)", "language": "python", "name": "learn-env_2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}}, "nbformat": 4, "nbformat_minor": 4}